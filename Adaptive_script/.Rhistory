mu_real = L %*% rnorm(n) +  (grid[, 1] - .5 + grid[, 2] - .5) * 1
plotf2d(mu_posterior, "Kriging surface", -3, 3)
estd = sqrt(diag(Sigma_posterior))
plotf2d(estd, "Predication standard error", 0, 0.2)
plotf2d(mu_real, "True field", -3, 3)
library(pracma)
plotf2d <- function(v, string, vmin, vmax){
vv <- v
dim(vv) <- c(n1, n2)
levelplot(vv, col.regions = coul, main = string, at=seq(vmin, vmax, length.out=100), xlab = "s1", ylab = "s2")
}
# # Setup the grid
n1 = 25 # number of grid points along east direction
n2 = 25 # number of grid points along north direction
n = n1 * n2 # total number of grid points
dn1 = 1/n1
dn2 = 1/n2
sites1 = array(seq(0, 1, dn1), c(n1, 1))
sites2 = array(seq(0, 1, dn2), c(n2, 1))
dn1 = 1 / n1
dn2 = 1 / n2
sites1 = array(seq(0, 1, dn1), c(n1, 1))
sites2 = array(seq(0, 1, dn2), c(n2, 1))
sites1v= matrix(meshgrid(sites1, sites2)$X, ncol = 1)
sites2v = matrix(meshgrid(sites1, sites2)$Y, ncol = 1)
grid = cbind(sites1m, sites2m)
sigmah = stats[[iter]][2]
phih = stats[[iter]][3]
tauh = stats[[iter]][4]
thetah = c(sigmah, phih, tauh)
alphah = stats[[iter]][5]
mu_prior = (grid[, 1] - .5 + grid[, 2] - .5) * alphah
# distance matrix for the grid locs
HGrid = rdist(grid, grid)
C0 = MaternCov(thetah, HGrid)
# distance matrix for observation locs
HObs = rdist(s, s)
C = MaternCov(thetah, HObs) + tau ** 2 * matrix(rep(1, N * N), c(N, N))
# distance matrix for observation locs and variable locs
HGridObs = rdist(grid, s)
C0_ = MaternCov(thetah, HGridObs)
mu_posterior = mu_prior + C0_ %*% solve(C) %*% (y - mu * alphah)
Sigma_posterior = C0 - C0_ %*% solve(C) %*% t(C0_)
L = t(chol(C0))
mu_real = L %*% rnorm(n) +  (grid[, 1] - .5 + grid[, 2] - .5) * 1
plotf2d(mu_posterior, "Kriging surface", -4, 4)
estd = sqrt(diag(Sigma_posterior))
plotf2d(estd, "Predication standard error", 0, 0.1)
plotf2d(mu_real, "True field", -4, 4)
library(pracma)
plotf2d <- function(v, string, vmin, vmax){
vv <- v
dim(vv) <- c(n1, n2)
levelplot(vv, col.regions = coul, main = string, at=seq(vmin, vmax, length.out=100), xlab = "s1", ylab = "s2")
}
# # Setup the grid
n1 = 25 # number of grid points along east direction
n2 = 25 # number of grid points along north direction
n = n1 * n2 # total number of grid points
dn1 = 1/n1
dn2 = 1/n2
sites1 = array(seq(0, 1, dn1), c(n1, 1))
sites2 = array(seq(0, 1, dn2), c(n2, 1))
dn1 = 1 / n1
dn2 = 1 / n2
sites1 = array(seq(0, 1, dn1), c(n1, 1))
sites2 = array(seq(0, 1, dn2), c(n2, 1))
sites1v= matrix(meshgrid(sites1, sites2)$X, ncol = 1)
sites2v = matrix(meshgrid(sites1, sites2)$Y, ncol = 1)
grid = cbind(sites1m, sites2m)
sigmah = stats[[iter]][2]
phih = stats[[iter]][3]
tauh = stats[[iter]][4]
thetah = c(sigmah, phih, tauh)
alphah = stats[[iter]][5]
mu_prior = (grid[, 1] - .5 + grid[, 2] - .5) * alphah
# distance matrix for the grid locs
HGrid = rdist(grid, grid)
C0 = MaternCov(thetah, HGrid)
# distance matrix for observation locs
HObs = rdist(s, s)
C = MaternCov(thetah, HObs) + tau ** 2 * matrix(rep(1, N * N), c(N, N))
# distance matrix for observation locs and variable locs
HGridObs = rdist(grid, s)
C0_ = MaternCov(thetah, HGridObs)
mu_posterior = mu_prior + C0_ %*% solve(C) %*% (y - mu * alphah)
Sigma_posterior = C0 - C0_ %*% solve(C) %*% t(C0_)
L = t(chol(C0))
mu_real = L %*% rnorm(n) +  (grid[, 1] - .5 + grid[, 2] - .5) * 1
plotf2d(mu_posterior, "Kriging surface", -4, 4)
estd = sqrt(diag(Sigma_posterior))
plotf2d(estd, "Predication standard error", 0, 1)
plotf2d(mu_real, "True field", -4, 4)
# library(pracma)
# plotf2d <- function(v, string, vmin, vmax){
#   vv <- v
#   dim(vv) <- c(n1, n2)
#   levelplot(vv, col.regions = coul, main = string, at=seq(vmin, vmax, length.out=100), xlab = "s1", ylab = "s2")
# }
#
# # # Setup the grid
# n1 = 25 # number of grid points along east direction
# n2 = 25 # number of grid points along north direction
# n = n1 * n2 # total number of grid points
#
# dn1 = 1/n1
# dn2 = 1/n2
# sites1 = array(seq(0, 1, dn1), c(n1, 1))
# sites2 = array(seq(0, 1, dn2), c(n2, 1))
#
# dn1 = 1 / n1
# dn2 = 1 / n2
# sites1 = array(seq(0, 1, dn1), c(n1, 1))
# sites2 = array(seq(0, 1, dn2), c(n2, 1))
#
# sites1v= matrix(meshgrid(sites1, sites2)$X, ncol = 1)
# sites2v = matrix(meshgrid(sites1, sites2)$Y, ncol = 1)
# grid = cbind(sites1m, sites2m)
#
# sigmah = stats[[iter]][2]
# phih = stats[[iter]][3]
# tauh = stats[[iter]][4]
# thetah = c(sigmah, phih, tauh)
# alphah = stats[[iter]][5]
# mu_prior = (grid[, 1] - .5 + grid[, 2] - .5) * alphah
#
#
# # distance matrix for the grid locs
# HGrid = rdist(grid, grid)
# C0 = MaternCov(thetah, HGrid)
# # distance matrix for observation locs
# HObs = rdist(s, s)
# C = MaternCov(thetah, HObs) + tau ** 2 * matrix(rep(1, N * N), c(N, N))
# # distance matrix for observation locs and variable locs
# HGridObs = rdist(grid, s)
# C0_ = MaternCov(thetah, HGridObs)
#
# mu_posterior = mu_prior + C0_ %*% solve(C) %*% (y - mu * alphah)
# Sigma_posterior = C0 - C0_ %*% solve(C) %*% t(C0_)
#
# L = t(chol(C0))
# mu_real = L %*% rnorm(n) +  (grid[, 1] - .5 + grid[, 2] - .5) * 1
#
# plotf2d(mu_posterior, "Kriging surface", -4, 4)
# estd = sqrt(diag(Sigma_posterior))
# plotf2d(estd, "Predication standard error", 0, 1)
# plotf2d(mu_real, "True field", -4, 4)
par(mfrow=c(1, 2))
set.seed(0421)
# sample locations
N = 200
s = matrix(runif(2*N), ncol=2)
plot(s[,1], s[,2], pch=4,
main="Observation sites", xlab="", ylab="")
# Distance matrix
# (without packages, since only one-time cost)
H = matrix(0, ncol=N, nrow=N)
for (i in 1:N){
for (j in 1:N){
H[i,j] = sqrt((s[i,1]-s[j,1])**2+(s[i,2]-s[j,2])**2)
}
}
# parameters
sigma = 1.0
phi = 10.0
tau = 0.05
theta = c(sigma, phi, tau)
alpha = 1.0
# Function:
# assembling of covariance matrix
# with Matern-like ansatz
# Input:
# theta=(sigma, phi, tau) with parameters
# Output:
# cov matrix in size of distance matrix H
MaternCov <- function(theta, H){
sigma=theta[1]
phi=theta[2]
tau=theta[3]
Cov = sigma**2*(1+phi*H)*exp(-phi*H)
return(Cov)
}
# Covariance matrix
Cov = MaternCov(theta, H)
# simulation
LCov = t (chol(Cov))
y0 = LCov %*% rnorm(N)
mu = ((s[,1]-0.5)+(s[,2]-0.5))
y = y0 + alpha*mu + rnorm(N, mean=0, sd=tau)
# generating colormap
rbPal <- colorRampPalette(c('blue', 'green','yellow'))
cols <- rbPal(N)[as.numeric(cut(y,breaks = N))]
cexs_scale = 0.4
cexs_shift = 3.0
cexs = pmax(0, cexs_scale*(y+cexs_shift))
# plotting
plot(s[,1],s[,2], col = cols, cex=cexs, pch=4,
main="Observations", xlab="", ylab="")
a = matrix(c(2, 3, 4, 5), nrow = 2)
a
a = matrix(c(2, 3, 4, 5, 7:14), nrow = 2)
a
a = matrix(c(2, 3, 4, 5, 7:14), nrow = 5)
a = matrix(c(2, 3, 4, 5, 7:14), ncol = 2)
a
b = a[2:]
b = a[2:end]
b = a[2:6]
b
b = a[2:6, ]
b
source('~/OneDrive - NTNU/NTNU/2021/MA8702/Exams/MCMC.R')
source('~/OneDrive - NTNU/NTNU/2021/MA8702/Exams/MCMC.R')
source('~/OneDrive - NTNU/NTNU/2021/MA8702/Exams/MCMC.R')
source('~/OneDrive - NTNU/NTNU/2021/MA8702/Exams/MCMC.R')
source('~/OneDrive - NTNU/NTNU/2021/MA8702/Exams/MCMC.R')
source('~/OneDrive - NTNU/NTNU/2021/MA8702/Exams/MCMC.R')
source('~/OneDrive - NTNU/NTNU/2021/MA8702/Exams/MCMC.R')
source('~/OneDrive - NTNU/NTNU/2021/MA8702/Exams/MCMC.R')
source('~/OneDrive - NTNU/NTNU/2021/MA8702/Exams/MCMC.R')
source('~/OneDrive - NTNU/NTNU/2021/MA8702/Exams/MCMC.R')
source('~/OneDrive - NTNU/NTNU/2021/MA8702/Exams/MCMC.R')
??dnorm
?dnorm
source('~/OneDrive - NTNU/NTNU/2021/MA8702/Exams/MCMC.R')
source('~/OneDrive - NTNU/NTNU/2021/MA8702/Exams/MCMC.R')
abline(0, 1)
plot(rnorm(100))
abline(0, 1)
??abline
?'ablie'
?'abline'
dir()
---
title: "MA8701 Advanced methods in statistical inference and learning"
author: "Mette Langaas IMF/NTNU"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
html_document:
toc: yes
toc_float: yes
code_download: yes
toc_depth: 3
beamer_presentation:
slide_level: 1
keep_tex: yes
latex_engine: xelatex
pdf_document:
toc: yes
toc_depth: 3
latex_engine: xelatex
subtitle: 'L12 with Kjersti Aas'
bibliography: ../Part3/ref.bib
---
```{r setup, include=FALSE,echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE,warning = FALSE, error = FALSE)
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(iml))
suppressPackageStartupMessages(library(pdp))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(glmnet))
suppressPackageStartupMessages(library(glmnet))
suppressPackageStartupMessages(library(randomForest))
suppressPackageStartupMessages(library(pROC)) #for ROC curves
suppressPackageStartupMessages(library(corrplot)) #for ROC curves
suppressPackageStartupMessages(library(correlation)) #for ROC curves
knitr::opts_chunk$set(echo = TRUE, message=FALSE,warning = FALSE, error = FALSE)
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(iml))
# Part 4: Explainable AI
The main role of this note is to give the R code for the examples found on the Part 4 slide sets, and detailed references for further reading.
## Why a part on XAI?
In **Part 1** we worked with _interpretable_ methods:
* linear regression (LS/MLE, ridge and lasso)
* logistic regression (MLE, ridge and lasso)
By studying the estimated regression coefficients we could (to some extent) explain what our fitted model could tell us about the data we had analysed.
In **Part 2** we started by studying a classification and regression tree, which is also an interpretable method.
See Chapter 4.1, 4.2 and 4.4 of @molnar2019 (Chapther 4_ Interpretable Models, 4.1 Linear regression, 4.2 Logistic regression and 4.4 Decision tree) on a discussion around what to report or plot from interpretable methods.
In Part 2 we then moved on to different versions of ensemble methods (bagging, random forest, xgboost, superlearner) - which are not interpretable. In **Part 3** we studied artificial neural networks (deep nets, recurrent nets, Bayesian nets) - again not interpretable methods.
We may refer to the methods of Part 2 and 3 as _black box_ methods, since in a prediction setting we would input an observation to the fitted method and the method would output a prediction - but we would not have a specific formula that we use to explain why the method gave this prediction.
In many situations we would like to know more about the model that the method have fitted. We would like some kind of interpretation of what the underlying methods does, for example:
* what is the mathematical relationship between $x$ and $y$ in the fitted method?
* how much of the variability in the data is explained by feature $x$ in the fitted method?
* is there an interaction effect between $x_1$ and $x_2$ in the fitted method?
Remark: we want to interpret the fitted method, based on the available data (but not really interpret directly the data).
We would also like to _explain_ the prediction for a given input.
See Chapter 2 of @molnar2019 on a discussion of _interpretability_.
## Reading list
* @molnar2019: Chapters 2, 5 (not 5.8), 6.1
* Four slide sets from Kjersti Aas (**sent to participants by email**)
+ Introduction
+ LIME
+ Shapley values
+ Counterfactual explanations
Supplementary reading is specified for (below).
## Outline
We start by motivating the need for XAI, and then look at
* Global explanation methods
+ Model specific methods
+ Model agnostic methods (PDP plots, ICE plots, ALE plots)
* Local explanation methods
+ Method specific
+ Model agnostic (LIME, Shapley values, Counterfactual explanations)
## Plan
* L12: Two slide sets - Introduction and LIME
* L13: Two slide sets - Shapley values and Counterfactuals
# L12: Introduction slide set
## Analysis of the bike data
### Linear model
```{r}
# download manually
#"https://github.com/christophM/interpretable‐ml‐book/blob/master/data/bike.Rdata"
load("bike.Rdata")
colnames(bike)
n=dim(bike)[1]
bikeTrain=bike[1:600,]
bikeTest<-bike[601:n,]
linearMod <- lm(cnt~.,data=bikeTrain) #bikeTrain
tmp <- summary(linearMod)
head(bike)
linearMod <- lm(cnt~.,data=bikeTrain) #bikeTrain
tmp <- summary(linearMod)
tmp$r.square
tmp$coefficients[rev(order(abs(tmp$coefficients[,3]))),]
corrplot(cor(bikeTrain[,8:11]))
library("relaimpo")
install.packages("relaimpo")
library("relaimpo")
calc.relimp(cnt~., data=bikeTrain|,-6], type="lmg",rela=TRUE)
rev(sort(crf$lmg))
calc.relimp(cnt~., data=bikeTrain|,-6], type="lmg",rela=TRUE)
rev(sort(crf$lmg))
calc.relimp(cnt~., data=bikeTrain[,-6], type="lmg",rela=TRUE)
rev(sort(crf$lmg))
library(xgboost)
install.packages("xgboost")
library(xgboost)
n<-dim(bike)[1]
bikeTrain<-bike[1:600,]
bikeTest<-bike[601:n,]
xgb.train=xgb.DMatrix(data = as.matrix(sapply(bikeTrain[,-11], as.numeric)),label = bikeTrain[,"cnt"])
xgb.test<-xgb.DMatrix(data = as.matrix(sapply(bikeTest[,-11], as.numeric)),label = bikeTest[,"cnt"])
params<-list(eta = 0.1,
objective = "reg:squarederror",
eval_metric = "rmse",
tree_method="hist") # gpu_hist
#RNGversion(vstr = "3.5.0")
set.seed(12345)
model<-xgb.train(data = xgb.train,
params = params,
nrounds = 50,
print_every_n = 10,
ntread = 5,
watchlist = list(train = xgb.train,
test = xgb.test),
verbose = 1)
xgb.importance(model=model)
# 1. create a data frame with just the features
features<-bikeTrain[,-11]
# 2. Create a vector with the actual responses
response<-bikeTrain[,"cnt"]
# 3. Create custom predict function that returns the predicted values as a vector
pred<-function(model, newdata)
{
#xgb.test<-xgb.DMatrix(data = as.matrix(sapply(newdata[,‐11], as.numeric)),label = newdata[,11])
xgb.test<-xgb.DMatrix(data = as.matrix(sapply(newdata, as.numeric)))
results<-predict(model,newdata=xgb.test)
#return(results[[3L]])
return(results)
}
#4. Define predictor
predictor.xgb<-Predictor$new(
model = model,
data = features,
y = response,
predict.fun = pred,
class = "regression"
)
#4. Define predictor
predictor.xgb<-Predictor$new(
model = model,
data = features,
y = response,
predict.fun = pred,
class = "regression"
)
#5. Compute feature effects
eff<-FeatureEffect$new(predictor.xgb, feature = "temp", method="ale")
plot(eff)
eff<-FeatureEffects$new(predictor.xgb, method="ale")
eff$plot()
#4. Define predictor
predictor.xgb<-predictor$new(
model = model,
data = features,
y = response,
predict.fun = pred,
class = "regression"
)
#4. Define predictor
predictor.xgb<-predictor$new(
model = model,
data = features,
y = response,
predict.fun = pred,
class = "regression"
)
#4. Define predictor
predictor.xgb<-pred$new(
model = model,
data = features,
y = response,
predict.fun = pred,
class = "regression"
)
#4. Define predictor
predictor.xgb<-predictor.xgb$new(
model = model,
data = features,
y = response,
predict.fun = pred,
class = "regression"
)
#4. Define predictor
predictor.xgb<-xgb$new(
model = model,
data = features,
y = response,
predict.fun = pred,
class = "regression"
)
library(lime)
library(ranger)
install.packages("lime")
install.packages("ranger")
library(lime)
library(ranger)
predict_model.ranger <- function(x,newdata,type)
{
pred.rf <- predict(x, data = newdata)
switch(
type,
raw = data.frame(Response = res$class, stringsAsFactors = FALSE),
prob = as.data.frame(pred.rf$predictions[,2])
)
}
model_type.ranger <- function(x, ...)
{
'regression'
}
model<- ranger(cnt ~ ., data = bikeTrain, num.trees = 50, num.threads = 6,
verbose = TRUE,
probability = FALSE,
importance = "impurity",
mtry = sqrt(27))
print(model)
explainer <- lime::lime(
bikeTrain,
model = model,
#bin_continuous = FALSE
bin_continuous = TRUE,
n_bins = 10,
quantile_bins=TRUE
)
explanationLime <- explain(
bikeTest[10:14,-11],
explainer = explainer,
#n_labels = 1,
n_features = 5,
n_permutations = 5000,
feature_select = "auto",
kernel_width = 3)
explanationLime <- explain(
bikeTest[10:14,-11],
explainer = explainer,
#n_labels = 1,
n_features = 5,
n_permutations = 5000,
feature_select = "auto",
kernel_width = 3)
lime::plot_features(explanationLime,
ncol = 2)
install.packages("randomForest")
print("hello world")
load("data.txt")
setwd("~/OneDrive - NTNU/MASCOT_PhD/Missions/Adaptive_script")
load("data.txt")
data <- read.table("data.txt", header = F)
print(data)
View(data)
data <- read.table("data.txt", delimiter = ",", header = F)
data <- read.table("data.txt", sep = ",", header = F)
View(data)
data <- read.table("dataFit.txt", sep = ",", header = F)
View(data)
sal_sinmod = data[:, 0]
sal_sinmod = data(:, 0)
sal_sinmod = data[, ]
sal_sinmod = data[, 1]
sal_sinmod
plot(sal_sinmod, sal_auv)
sal_sinmod = data[, 1]
sal_auv = data[, 3]
temp_sinmod = data[, 2]
temp_auv = data[, 4]
plot(sal_sinmod, sal_auv)
md = lm(sal_auv ~ sal_sinmod)
summary(md)
abline(0, 1)
